{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dcb96b06-519b-48ba-95f0-b90013cc4c2b",
      "metadata": {
        "id": "dcb96b06-519b-48ba-95f0-b90013cc4c2b"
      },
      "source": [
        "# Reference\n",
        "* https://towardsdatascience.com/feature-extraction-with-bert-for-text-classification-533dde44dc2f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcabf54b-ddac-4a14-bc42-6d86523c4a1d",
      "metadata": {
        "id": "bcabf54b-ddac-4a14-bc42-6d86523c4a1d"
      },
      "outputs": [],
      "source": [
        "#https://drive.google.com/file/d/1Rn5yjsOpA6MGNvFnmarIcD3Bb2u2wniO/view?usp=share_link\n",
        "#!gdown -q 1Rn5yjsOpA6MGNvFnmarIcD3Bb2u2wniO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee47f6ce-f3f0-43c6-a4b4-15cd864f469e",
      "metadata": {
        "id": "ee47f6ce-f3f0-43c6-a4b4-15cd864f469e"
      },
      "outputs": [],
      "source": [
        "import numpy as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import random\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39dc32d7-829d-48c7-ac7f-1c58c3a86ca5",
      "metadata": {
        "id": "39dc32d7-829d-48c7-ac7f-1c58c3a86ca5"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"bbc-news.csv\")\n",
        "df_train, df_test = train_test_split(df, test_size=0.25, random_state=2023)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a38d2968-3315-425b-b0ae-b3379bb66005",
      "metadata": {
        "id": "a38d2968-3315-425b-b0ae-b3379bb66005",
        "outputId": "e451fcb2-e773-45a6-dcbe-360f49993eef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 1668 entries, 668 to 855\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   text    1668 non-null   object\n",
            " 1   label   1668 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 39.1+ KB\n"
          ]
        }
      ],
      "source": [
        "df_train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bb0fff1-c3f8-41bf-977f-5754b103685c",
      "metadata": {
        "id": "3bb0fff1-c3f8-41bf-977f-5754b103685c",
        "outputId": "de714ba6-14cb-452a-fb1e-db1bc3e4fd33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 557 entries, 294 to 464\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   text    557 non-null    object\n",
            " 1   label   557 non-null    object\n",
            "dtypes: object(2)\n",
            "memory usage: 13.1+ KB\n"
          ]
        }
      ],
      "source": [
        "df_test.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "def1792c-2a70-4146-907e-8ab064cc4987",
      "metadata": {
        "id": "def1792c-2a70-4146-907e-8ab064cc4987",
        "outputId": "e07b0ff7-cc42-4829-e4b0-a58dac66d7a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[515, 1466, 604]\n",
            "====================================================================================================\n",
            "Some samples from the dataset\n",
            "====================================================================================================\n",
            "text\n",
            "--------------------------------------------------\n",
            "Ray Charles studio becomes museum A museum dedicated to the career of the late legendary singer Ray Charles is to open in his former recording studio in Los Angeles. His longtime publicist Jerry Digney said the museum would house \"archive materials from recordings, to awards, to ephemera, to wardrobe\". A tour bus used by Charles and his entourage over the years will also be on permanent display. It is hoped the museum will be ready for visitors in late 2007. Mr Digney said the recording studio and offices had been used by Charles for many years, and was where he recorded much of his last album, Genius Loves Company. It is hoped the museum will also house an education centre. The building had been declared a historic landmark by the city of Los Angeles just before Charles' death in June 2004 at the age of 73. Following his death, Charles won eight Grammy Awards, including album of the year for Genius Loves Company, a collection of duets.\n",
            "\n",
            "--------------------------------------------------\n",
            "label:  entertainment\n",
            "text\n",
            "--------------------------------------------------\n",
            "Vickery out of Six Nations England tight-head prop Phil Vickery has been ruled out of the rest of the 2005 RBS Six Nations after breaking a bone in his right forearm. Vickery was injured as his club side, Gloucester, beat Bath 17-16 in the West country derby on Saturday. He could be joined on the sidelines by Bath centre Olly Barkley, who sat out the derby due to a leg injury. Barkley will have a scan on Sunday and might miss England's trip to Six Nations leaders Ireland next weekend. The news is just the latest blow for coach Andy Robinson, who has seen his side lose their opening two matches in the 2005 Six Nations. Robinson is already without World Cup winners Jonny Wilkinson, Will Greenwood, Mike Tindall, Richard Hill and Trevor Woodman through injury. Vickery has broken the radius, a large bone in his forearm. He only returned to the England side last weekend after a long-term back injury, which was followed by a fractured eye socket. And the Gloucester prop was only recalled after Leicester tight-head Julian White suffered a neck injury which has already seen him ruled out of the Ireland game. Bath prop Matt Stevens is the only remaining tight-head in England's training squad and could be involved against Ireland. But he has to play second fiddle at club level to Duncan Bell, who excelled for England A against France and may now be called into the squad. The extent of Barkley's injury is not yet clear but Bath boss John Connolly rates him no better than \"50-50\" to face Ireland. Barkley played at inside cente in England's defeat by France and if he is unable to play, England's constantly-changing midfield will once again have to be altered. Robinson could choose to recall Mathew Tait or Henry Paul, although Tait endured a nightmare for Newcastle against Leicester on Saturday and Paul limped off with an ankle injury against Bath. In-form Leicester centre Ollie Smith is the other outstanding candidate, and two tries against Newcastle will have boosted his chances. Fly-half Andy Goode is also a strong contender for the match-day 22 after an immaculate kicking display on Saturday. England, fourth in the Six Nations table with zero points, play Ireland, top of the table, in Dublin on 27 February, kick-off 1500 GMT.\n",
            "\n",
            "--------------------------------------------------\n",
            "label:  sport\n",
            "text\n",
            "--------------------------------------------------\n",
            "IBM puts cash behind Linux push IBM is spending $100m (Â£52m) over the next three years beefing up its commitment to Linux software. The cash injection will be used to help its customers use Linux on every type of device from handheld computers and phones right up to powerful servers. IBM said the money will fund a variety of technical, research and marketing initiatives to boost Linux use. IBM said it had taken the step in response to greater customer demand for the open source software. In 2004 IBM said it had seen double digit growth in the number of customers using Linux to help staff work together more closely. The money will be used to help this push towards greater collaboration and will add Linux-based elements to IBM's Workplace software. Workplace is a suite of programs and tools that allow workers to get at core business applications no matter what device they use to connect to corporate networks. One of the main focuses of the initiative will be to make it easier to use Linux-based desktop computers and mobile devices with Workplace. Even before IBM announced this latest spending boost it was one of the biggest advocates of the open source way of working. In 2001 it put $300m into a three-year Linux program and has produced Linux versions of many of its programs. Linux and the open source software movement are based on the premise that developers should be free to tinker with the core components of software programs. They reason that more open scrutiny of software produces better programs and fuels innovation.\n",
            "\n",
            "--------------------------------------------------\n",
            "label:  tech\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "train_text, train_label = list(df_train['text']), list(df_train['label'])\n",
        "test_text, test_label = list(df_test['text']), list(df_test['label'])\n",
        "nshowed = 3\n",
        "idx_showed = random.sample(range(len(train_text)), nshowed)\n",
        "\n",
        "print(idx_showed)\n",
        "print(\"=\"*100)\n",
        "print(\"Some samples from the dataset\")\n",
        "print(\"=\"*100)\n",
        "for idx in idx_showed:\n",
        "    print(\"text\");\n",
        "    print('-'*50)\n",
        "    print(train_text[idx])\n",
        "    print()\n",
        "    print('-'*50)\n",
        "    print(\"label: \", train_label[idx])\n",
        "print(\"=\"*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1576df5-b661-429e-9f53-a064552cd709",
      "metadata": {
        "id": "b1576df5-b661-429e-9f53-a064552cd709",
        "outputId": "3a58f954-8357-45e7-83d6-99159b7ce8e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer's Type:  <class 'transformers.models.distilbert.tokenization_distilbert_fast.DistilBertTokenizerFast'>\n",
            "model's Type:  <class 'transformers.models.distilbert.modeling_distilbert.DistilBertModel'>\n",
            "\n",
            "<class 'dict'>\n",
            "dict_keys(['input_ids', 'attention_mask'])\n",
            "train_tokenized['input_ids'].shape:  torch.Size([1668, 512])\n",
            "train_tokenized['attention_mask'].shape:  torch.Size([1668, 512])\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"distilbert-base-uncased\").to(device)\n",
        "\n",
        "print(\"Tokenizer's Type: \", type(tokenizer))\n",
        "print(\"model's Type: \", type(model))\n",
        "print()\n",
        "train_tokenized = tokenizer(train_text, padding = True, truncation = True, return_tensors=\"pt\")\n",
        "test_tokenized = tokenizer(test_text, padding = True, truncation = True, return_tensors=\"pt\")\n",
        "train_tokenized = {k:v.to(device) for k,v in train_tokenized.items()} # use GPU if possible\n",
        "test_tokenized = {k:v.to(device) for k,v in test_tokenized.items()} # use GPU if possible\n",
        "\n",
        "print(type(train_tokenized))\n",
        "print(train_tokenized.keys())\n",
        "print(\"train_tokenized['input_ids'].shape: \", train_tokenized['input_ids'].shape)\n",
        "print(\"train_tokenized['attention_mask'].shape: \", train_tokenized['attention_mask'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c80afe1-854a-4189-be79-ca370cb631f0",
      "metadata": {
        "id": "7c80afe1-854a-4189-be79-ca370cb631f0"
      },
      "outputs": [],
      "source": [
        "def get_features(tokenized_set, model, batch_size=16):\n",
        "    features = []\n",
        "    nsamples, feature_dim = tokenized_set['input_ids'].shape\n",
        "    nbatches = nsamples//batch_size\n",
        "    start_idx = 0\n",
        "    for idx in range(nbatches):\n",
        "        end_idx = min(start_idx + batch_size, nsamples)\n",
        "        if idx == (nbatches - 1): # last batch\n",
        "            end_idx = nsamples\n",
        "        input_ids = tokenized_set['input_ids'][start_idx:end_idx, ...]\n",
        "        attention_mask = tokenized_set['input_ids'][start_idx:end_idx, ...]\n",
        "        with torch.no_grad():\n",
        "            feature = model(input_ids, attention_mask)\n",
        "            #features.append(feature.last_hidden_state[:, 0, :]) #get only the [CLS] hidden states\n",
        "            features.append(feature.last_hidden_state.mean(dim=1))\n",
        "        #\n",
        "        start_idx += batch_size\n",
        "    return torch.cat(features, dim=0)\n",
        "\n",
        "train_features = get_features(train_tokenized, model, batch_size=16)\n",
        "test_features = get_features(test_tokenized, model, batch_size=16)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95812a35-ca32-4d70-8d8e-50fd803e392a",
      "metadata": {
        "id": "95812a35-ca32-4d70-8d8e-50fd803e392a",
        "outputId": "9c6bba32-b513-4af0-a51f-d7b91ee81d69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training size: torch.Size([1668, 768])\n",
            "testing size: torch.Size([557, 768])\n"
          ]
        }
      ],
      "source": [
        "print(\"training size:\", train_features.shape)\n",
        "print(\"testing size:\", test_features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8834c789-134b-472d-8966-32847c2865c0",
      "metadata": {
        "id": "8834c789-134b-472d-8966-32847c2865c0",
        "outputId": "7cbc0bb4-570f-45ff-b3a2-2a25e78f6b14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set (nsamples | ndim): 1668 | 768\n",
            "Testing set (nsamples | ndim): 557 | 768\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9748653500897666"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "X_train = train_features.to(\"cpu\").numpy()\n",
        "X_test = test_features.to(\"cpu\").numpy()\n",
        "\n",
        "N_train, ndim = X_train.shape\n",
        "N_test, _ = X_test.shape\n",
        "lb_encoder = LabelEncoder().fit(train_label[:N_train])\n",
        "y_train = lb_encoder.transform(train_label[:N_train])\n",
        "y_test = lb_encoder.transform(test_label[:N_test])\n",
        "\n",
        "print(f\"Training set (nsamples | ndim): {N_train} | {ndim}\")\n",
        "print(f\"Testing set (nsamples | ndim): {N_test} | {ndim}\")\n",
        "\n",
        "#model = LogisticRegression( max_iter=1000)\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train,y_train)\n",
        "model.score(X_test,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "177b6ff5-f2d2-433e-a925-ee7d95f0e4b7",
      "metadata": {
        "id": "177b6ff5-f2d2-433e-a925-ee7d95f0e4b7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}